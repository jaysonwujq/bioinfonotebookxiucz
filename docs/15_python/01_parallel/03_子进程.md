## 
### **subprocess.call()**
父进程等待子进程完成
返回退出信息(returncode，相当于exit code，见Linux进程基础)

### **subprocess.check_call()**
父进程等待子进程完成
返回0
检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try...except...来检查(见Python错误处理)。
 
### **subprocess.check_output()**
父进程等待子进程完成
返回子进程向标准输出的输出结果
检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try...except...来检查。

### **subprocess.Popen()**
实际上，我们上面的三个函数都是基于Popen()的封装(wrapper)。这些封装的目的在于让我们容易使用子进程。当我们想要更个性化我们的需求的时候，就要转向Popen类，该类生成的对象用来代表子进程。

+ 使用 Popen 可以在Python进程中创建子进程，
+ 如果只对子进程的执行退出状态感兴趣，可以调用 subprocess.call() 函数，
+ 如果想通过异常处理机制解决子进程异常退出的情形，可以考虑使用 subprocess.check_call() 和 subprocess.check_output;
+ 如果希望获得子进程的输出，可以调用 subprocess.check_output()，但 Popen() 无疑是功能最强大的。
subprocess模块的缺陷在于默认提供的父子进程间通信手段有限，只有管道；同时创建的子进程专门用来执行外部的程序或命令。

----
## **详解subprocess.Popen()**
1. 官方文档指出由于安全原因故不建议使用shell=True
```python
>>> p = subprocess.Popen('ls -l', shell=True) 
>>> p = subprocess.Popen(['ls', '-cl']) 
```
2. Popen 对象的属性
```python
p.returncode #该属性表示子进程的返回状态，returncode可能有多重情况：
#None —— 子进程尚未结束；
#==0 —— 子进程正常退出；
#> 0—— 子进程异常退出，returncode对应于出错码；
#< 0—— 子进程被信号杀掉了。

p.wait()
p.pid #子进程的PID。
```

Popen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的wait()方法，父进程才会等待 (也就是阻塞block)
```
import subprocess
child = subprocess.Popen(["ping","-c","5","www.google.com"])
print("parent process")
```
从运行结果中看到，父进程在开启子进程之后并没有等待child的完成，而是直接运行print。
 
对比等待的情况:
```
import subprocess
child = subprocess.Popen(["ping","-c","5","www.google.com"])
child.wait()
print("parent process")
```
----
多进程可以有效利用服务器多核CPU的计算资源，加速运行效率，在python中，通过内置模块multiprocessing来进行多进程编程。
子进程通过Process类来设置，示例如下
```
from  multiprocessing import Process
import subprocess
import shlex

def cal_seqs(fq):
    print('calculate fastq sequences')
    cnt = 0
    with open(fq) as f:
        for line in f:
            cnt += 1
    print('fastq sequences : {}'.format(cnt / 4))

def fastqc(fq):
    cmd = 'fastqc -t 10 {}'.format(fq)
    cmd_args = shlex.split(cmd)
    p = subprocess.Popen(cmd_args, stdout = subprocess.PIPE)
    p.stdout.read()

if __name__ == '__main__':
    fq = 'test.fq'
    p = Process(target = fastqc, args = (fq, ))
    p.start()
    cal_seqs(fq)
    print('Finish calculate fastq sequences')
```

上述代码**启动了一个子进程**来运行fastqc, 主进程则执行计算fastq文件的序列数，运行上述代码，你会看到类似如下的输出
```
calculate fastq sequences
fastq sequences : 100000
Finish calculate fastq sequences
Started analysis of test.fq
Approx 5% complete for test.fq
Approx 10% complete for test.fq
Approx 15% complete for test.fq
Approx 20% complete for test.fq
```
主进程中的行数计算先执行完毕，子进程后执行完毕， 说明子进程是非阻塞的。在主进程中启动了子进程后，主进程不管子进程是否运行结束，接着往下执行计算行数的结果，这就是非阻塞。如果需要等子进程执行完毕后，主进程再直接执行，也就是阻塞式的运行，需要join函数来进行阻塞，上述代码修改如下
```
from  multiprocessing import Process
import subprocess
import shlex

def cal_seqs(fq):
    print('calculate fastq sequences')
    cnt = 0
    with open(fq) as f:
        for line in f:
            cnt += 1
    print('fastq sequences : {}'.format(cnt / 4))

def fastqc(fq):
    cmd = 'fastqc -t 10 {}'.format(fq)
    cmd_args = shlex.split(cmd)
    p = subprocess.Popen(cmd_args, stdout = subprocess.PIPE)
    p.stdout.read()

if __name__ == '__main__':
    fq = 'test.fq'
    p = Process(target = fastqc, args = (fq, ))
    p.start()
    p.join()
    cal_seqs(fq)
    print('Finish calculate fastq sequences')
```
再次运行，可以看到如下输出
```
Started analysis of test.fq
Approx 5% complete for test.fq
Approx 10% complete for test.fq
...
Approx 95% complete for test.fq
Approx 100% complete for test.fq
calculate fastq sequences
fastq sequences : 100000
Finish calculate fastq sequences
```
join的作用就是阻塞子程序，等待子进程执行完毕之后，主进程才可以接着往下执行。如果只是需要执行某些程序，而且下文中也不依赖其结果，可以选择非阻塞式的运行，如果下文需要依赖其结果，就需要阻塞式的运行了，应该根据实际情况，灵活进行选择。

对于多个样本的重复处理，可以用多进程达到并行的目的，代码示例如下
```
process_list = []
samples = ['control1', 'control2', 'control3', 'case1', 'case2', 'case3']
for sample in sample:
    fq = '{}.fq'.format(sample)
    p = Process(target = fastqc, args = (sample, ))
    p.start()
    process_list.append(p)

# 根据需要选择是否阻塞
for p in process_list:
    p.join()
上述代码有一个问题，会一次性将所有的样本都并行，在实际开发中，我们需要结合计算资源来控制最大的并行数，此时可以使用进程池Pool类，可以方便的指定最大进程数，示例如下

p = Pool(3)
process_list = []
samples = ['control1', 'control2', 'control3', 'case1', 'case2', 'case3']
for sample in sample:
    fq = '{}.fq'.format(sample)
    p.append_async(target = fastqc, args = (sample, ))

p.close()
p.join()
```
上述代码还可以用map方法进行改写，更加简便，改写之后的完整代码如下
```
from multiprocessing import Pool
import subprocess
import shlex

def fastqc(fq):
    cmd = 'fastqc -t 10 {}'.format(fq)
    cmd_args = shlex.split(cmd)
    p = subprocess.Popen(cmd_args, stdout = subprocess.PIPE)
    p.stdout.read()

if __name__ == '__main__':
    samples = ['control1.fq', 'control2.fq', 'control3.fq', 'case1.fq', 'case2.fq', 'case3.fq']
    with Pool(3) as p:
        samples = ['control1.fq', 'control2.fq', 'control3.fq', 'case1.fq', 'case2.fq', 'case3.fq']
        p.map(fastqc, samples)
    print('Finish')
```
以上就是python多进程的基本用法，除此以外，python还支持进程间通信以及共享变量，更高级的用法请查看官方的API文档。