+ 无监督是训练集没有标准答案；
  + 无监督学习是指对没有打标签的样本进行学习，目标是发现训练样本的中的结构性知识。在这里，学习的样本是没有标记的。
  + 常见的无监督学习算法有:聚类，PCA、生成对抗网络。 
+ 监督学习是训练集都有标准答案；
  + 监督学习是指对已经打好标签的训练样本进行学习，目标是对训练样本集之外的数据进行预测的学习方式。
  + 常见的监督学习算法有:支持向量机（SVM），KNN，决策树，朴素贝叶斯，逻辑回归。
+ 半监督学习是一小部分训练集有标准答案，大部分训练集没有标准答案；
  + 常见的半监督学习算法:TSVM，S3VM，生成式方法，半监督聚类。
+ 强化学习是给到奖励或惩罚的刺激，通过试错和反馈来进行学习。

## 极大似然法 最小二乘
### 
极大似然估计从字面上来理解可以拆成三个词，分别是“极大”、“似然”、“估计”，分别的意思如下：
极大:最大的概率
似然：看起来是这个样子的
估计：就是这个样子的
连起来就是，最大的概率看起来是这个样子的那就是这个样子的。怎么样，是不是很朴素？

极大似然法（the Principle of Maximum Likelihood）是由高斯和费希尔先后提出的，这个方法的基础是极大似然原理。
极大似然法的原理是：样本所展现的状态就是所有可能状态中出现概率最大的那个状态。

### 与最小二乘的区别？
1）最小二乘是求计算值与实际值的欧式距离最小的参数，是从lost function的角度去看的。而极大似然是求目前这个观测数据出现概率最大的参数，是从概率的角度去看。
2）极大似然是要有分布假设的，而最小二乘没有这个假设。
3）当极大似然的分布假设为高斯分布的时候，是和最小二乘法等价的。

## 逻辑回归和线性回归
https://www.jianshu.com/p/eb6de2925a57

## 衡量模型好坏
+ 准确率 = 选出的正确信息条数 / 选出的信息总条数
+ 召回率 = 选出正确信息条数 / 样本中的信息条数

举个栗子，我们有100个苹果有90个是好的，10个是坏的。
我们弄了一个算法来找出坏的苹果。我们判断85个是好的，15个是坏的。其中85个好的里面，有2个是坏的；15个坏的里面有7个是好的。
准确率= 8/15 = 53.33%
召回率= 8/10 = 80%

+ F值（F-Score）= 准确率召回率2/(准确率+召回率)

上面选苹果的例子，F值= 0.53330.82 /(0.5333+0.8) = 63.99%

+ ROC和AUC


https://mooc1-1.chaoxing.com/course/207635776.html